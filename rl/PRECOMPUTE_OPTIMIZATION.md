# Pre-Compute All States Optimization

## Problem

In RL training, episode resets were taking **30 minutes each** because states were being recomputed from scratch for every episode:
- Each episode: 60 days Ã— 925 stocks = 55,500 state computations
- Each state requires a transformer forward pass through the frozen predictor
- Result: 30 minutes per episode Ã— 1000 episodes = **500 hours (21 days)** of wasted computation!

## Optimization Strategy

**Key insight**: We only need **recent data** for effective RL training!

- Using **last 4 years only** (~1000 trading days) instead of all 25 years (~6281 days)
- Recent market dynamics are most relevant for learning trading strategies
- Reduces precomputation from 3.5 hours â†’ ~20 minutes (~10x faster!)

## Solution

Pre-compute states for **ALL trading days once** at initialization, then reuse them across all episodes.

### Implementation

1. **New parameter in `TradingEnvironment.__init__()`:**
   ```python
   env = TradingEnvironment(
       data_loader=data_loader,
       agent=agent,
       precompute_all_states=True  # Enable optimization
   )
   ```

2. **One-time precomputation:**
   - Runs once during environment initialization
   - Computes states for all ~6000 trading days
   - Stores in CPU RAM cache (~32 GB)
   - Takes ~30 minutes once

3. **Fast episode resets:**
   - Episode reset just samples dates and reads from cache
   - **<0.01 seconds** instead of 30 minutes!
   - No redundant transformer forward passes

### Performance Impact

**For 1000 episodes:**

| Method | Time | Calculation |
|--------|------|-------------|
| Without optimization | **500 hours** | 1000 Ã— 30 min |
| With optimization | **~30 minutes** | 30 min (one-time) + 1000 Ã— 0.01s |
| **Speedup** | **~1000x faster** | |

### Memory Usage

- **Estimated:** ~32 GB RAM for full cache
- States stored on CPU RAM (not GPU)
- Formula: `num_days Ã— avg_stocks_per_day Ã— state_dim Ã— 4 bytes`
- Example: `6000 Ã— 900 Ã— 1469 Ã— 4 = ~31.8 GB`

### When to Use

**Use `precompute_all_states=True` when:**
- Training for many episodes (>10)
- You have sufficient RAM (~32-64 GB)
- Episode reset time is a bottleneck

**Don't use when:**
- Single episode testing
- Limited RAM (<32 GB)
- Using very large datasets (>10K stocks)

### Code Changes

**TradingEnvironment initialization (rl/rl_environment.py):**
```python
def __init__(self, ..., precompute_all_states: bool = False):
    # ...existing code...

    if precompute_all_states:
        print(f"\nðŸ”„ Pre-computing states for ALL {len(self.trading_days)} trading days...")
        self._precompute_all_states()
        print(f"âœ… Pre-computation complete!")
```

**Episode reset (rl/rl_environment.py):**
```python
def reset(self, start_date: Optional[str] = None):
    # ...select dates...

    # Check if states already cached
    all_dates_cached = all(date in self.state_cache for date in self.dates)

    if all_dates_cached:
        # Use pre-cached states (FAST!)
        print(f"   âœ… Using pre-cached states")
    else:
        # Compute states for this episode (SLOW)
        self._precompute_episode_states()
```

**Training script (rl/train_rl_phase2.py):**
```python
self.env = TradingEnvironment(
    data_loader=self.data_loader,
    agent=self.agent,
    # ...other params...
    precompute_all_states=True  # Enable optimization!
)
```

## Performance Results

### First Run (100 stocks, ~1000 trading days - last 4 years):

| Step | Time | Notes |
|------|------|-------|
| Feature preloading | ~1 minute | One-time HDF5 read into RAM (4 years only) |
| **OR** Load feature cache | <30 seconds | If `rl_feature_cache_4yr.h5` exists |
| State precomputation | ~20 minutes | 1.9s/day Ã— ~1000 days, transformer forward passes |
| **OR** Load state cache | <30 seconds | If `rl_state_cache_4yr.h5` exists |
| **Total initialization (first run)** | **~21 minutes** | **One-time cost** |
| **Total initialization (cached)** | **<1 minute** | **Loads from disk!** |
| Episode reset (after init) | <0.01s | Just reads from cache |
| Episode step | 0.001s | Cached states + prices |

### Subsequent Runs (with cached data):

| Step | Time | Notes |
|------|------|-------|
| Load feature cache | <30 seconds | From `data/rl_feature_cache_4yr.h5` |
| Load state cache | <30 seconds | From `data/rl_state_cache_4yr.h5` |
| **Total initialization** | **<1 minute** | **~20x faster than first run!** |

### vs Original Approach:

| Method | Time | Calculation |
|--------|------|-------------|
| **Old (per-episode precompute)** | **500 hours** | 30 min/episode Ã— 1000 episodes |
| **New (4yr + cache, first run)** | **21 minutes** | Feature load + state precompute (one-time) |
| **New (4yr + cache, cached)** | **<1 minute** | Load from disk |
| **Speedup vs old** | **~1400x faster!** | After first run |

### Testing

Run the test script to verify:
```bash
python rl/test_batched_states.py
```

Expected output:
- Feature preloading: ~5 minutes (for 100 stocks)
- State computation: ~1.9s per day
- Episode reset times: <0.01 seconds (uses cache)
- Episode steps: ~0.001s each (uses cache)

## Bottleneck Analysis

The state precomputation bottleneck was caused by:

1. **Sequential HDF5 reads** (Fixed by feature preloading)
   - Original: 900 HDF5 reads per date
   - Solution: Preload ALL features into RAM once
   - Speedup: ~1.8x

2. **Transformer forward passes** (Cannot be avoided)
   - Must compute predictor features for all dates
   - ~1.9s per day for 100 stocks with batch size 128
   - This is the fundamental cost - same as backtest_simulation

3. **Small mini-batches** (Fixed by increasing batch size)
   - Original: 32 stocks per batch
   - New: 128 stocks per batch
   - Reduces batch overhead

## Caching System

The optimization uses **two-level caching** to avoid recomputation:

### Level 1: Feature Cache (`data/rl_feature_cache_4yr.h5`)
- **What**: Raw features and prices from HDF5 dataset (last 4 years)
- **Size**: ~400 MB (for 100 stocks Ã— ~1000 days)
- **Creation time**: ~1 minute
- **Load time**: <30 seconds
- **Benefit**: Eliminates slow HDF5 reads

### Level 2: State Cache (`data/rl_state_cache_4yr.h5`)
- **What**: Precomputed state tensors (after transformer forward pass, last 4 years)
- **Size**: ~1.5 GB (for 100 stocks Ã— ~1000 days)
- **Creation time**: ~20 minutes
- **Load time**: <30 seconds
- **Benefit**: Eliminates expensive transformer forward passes

### Why 4 Years?

Using only the **last 4 years** of data provides several benefits:

1. **Speed**: ~10x faster precomputation (20 min vs 3.5 hours)
2. **Relevance**: Recent market dynamics are most predictive of future behavior
3. **Consistency**: Avoids regime changes from distant historical periods (2008 crisis, dot-com bubble, etc.)
4. **Sufficient data**: Still provides ~1000 trading days Ã— 100 stocks = 100K samples for training
5. **Smaller caches**: Only ~2 GB total vs ~10+ GB for 25 years

### Cache Invalidation

Caches need to be regenerated if:
- Dataset changes (features or prices updated)
- Model architecture changes (different hidden_dim, state_dim)
- Different stock universe (num_test_stocks changed)
- Time window changed (switching from 4 years to different period)

To regenerate: Simply delete the cache files (`rl_feature_cache_4yr.h5` and `rl_state_cache_4yr.h5`) and restart training.

## Related Optimizations

This builds on other performance optimizations:
1. **Feature caching** - Save/load raw features to skip HDF5 reads (5min â†’ <1min)
2. **State caching** - Save/load precomputed states to skip transformer passes (3.5hr â†’ <1min)
3. **Batched state extraction** - Process stocks in batches instead of sequentially
4. **Cached prices** - Eliminate redundant HDF5 reads during episodes
5. **Fast portfolio calculations** - Compute portfolio value once per step instead of 925 times
6. **Precompute all states once** - Reuse across all episodes instead of recomputing

Together, these optimizations reduce training time from **weeks to hours**!
