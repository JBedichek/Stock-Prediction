#!/usr/bin/env python3
"""
Offline critic pre-training with Monte Carlo returns.

Trains the critic network on synthetic data generated by programmatic strategy.
This gives the critic good initial value estimates before going online with the actor.

Training approach:
- Load synthetic data from generate_critic_data.py (with Monte Carlo returns)
- Train critic using supervised learning on actual episode returns (no bootstrapping)
- Save pre-trained critic checkpoint
- No actor training in this phase

Key advantage over TD learning:
- Uses actual cumulative returns from complete episodes (zero bias)
- No bootstrapping from poorly-initialized Q-estimates
- Better value estimates for jumpstarting online training
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import sys
import os
import pickle
from tqdm import tqdm
from typing import Dict

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from rl.rl_components import ActorCriticAgent, ReplayBuffer, compute_critic_loss, compute_critic_loss_monte_carlo


def pretrain_critic(
    data_path: str = 'data/critic_training_data.pkl',
    predictor_checkpoint: str = './checkpoints/best_model_100m_1.18.pt',
    save_path: str = './checkpoints/pretrained_critic.pt',
    batch_size: int = 256,
    num_epochs: int = 50,
    learning_rate: float = 3e-4,
    gamma: float = 0.99,
    tau: float = 0.005,
    device: str = 'cuda'
):
    """
    Pre-train critic on synthetic data using Monte Carlo returns.

    Trains critic to predict actual cumulative returns from episodes,
    rather than bootstrapping from its own estimates (TD learning).
    This provides zero-bias targets for better initial value estimates.

    Args:
        data_path: Path to synthetic data (pickle file with ReplayBuffer containing mc_return)
        predictor_checkpoint: Path to predictor checkpoint
        save_path: Where to save pre-trained critic
        batch_size: Batch size for training
        num_epochs: Number of training epochs
        learning_rate: Learning rate for critic optimizer
        gamma: Discount factor (not used for MC, but kept for compatibility)
        tau: Soft update coefficient for target network
        device: Device to use
    """
    print("\n" + "="*80)
    print("OFFLINE CRITIC PRE-TRAINING (Monte Carlo Returns)")
    print("="*80)

    # Load synthetic data
    print(f"\n1. Loading synthetic data from {data_path}...")
    with open(data_path, 'rb') as f:
        buffer = pickle.load(f)

    print(f"   ✅ Loaded {len(buffer)} transitions")
    buffer_stats = buffer.get_stats()
    print(f"   Buffer statistics:")
    print(f"   - Avg reward: {buffer_stats['avg_reward']:.4f}")
    print(f"   - Std reward: {buffer_stats['std_reward']:.4f}")

    # Initialize agent
    print("\n2. Initializing agent...")
    agent = ActorCriticAgent(
        predictor_checkpoint_path=predictor_checkpoint,
        state_dim=5881,  # 4 stocks × 1469 + 5 position encoding
        hidden_dim=1024,
        action_dim=5  # cash + 4 stocks
    ).to(device)

    # Freeze predictor and actor (only training critic)
    agent.feature_extractor.freeze_predictor()
    for param in agent.actor.parameters():
        param.requires_grad = False
    agent.actor.eval()

    # Critic in training mode
    agent.critic.train()
    agent.target_critic.eval()

    print(f"   ✅ Agent initialized")
    print(f"   - Feature extractor: FROZEN")
    print(f"   - Actor: FROZEN (not used)")
    print(f"   - Critic: TRAINING")

    # Optimizer for critic only
    optimizer = optim.AdamW(
        agent.critic.parameters(),
        lr=learning_rate,
        weight_decay=1e-4
    )

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs,
        eta_min=learning_rate * 0.1
    )

    # Training loop
    print(f"\n3. Training critic ({num_epochs} epochs, batch_size={batch_size})...")

    best_loss = float('inf')
    training_history = {
        'epoch_losses': [],
        'batch_losses': []
    }

    for epoch in range(num_epochs):
        epoch_losses = []

        # Calculate number of batches
        num_batches = len(buffer) // batch_size

        # Progress bar for batches
        pbar = tqdm(range(num_batches), desc=f"  Epoch {epoch+1}/{num_epochs}")

        for batch_idx in pbar:
            # Sample batch
            batch = buffer.sample(batch_size)

            # Compute loss (Monte Carlo - uses actual returns, no bootstrapping)
            loss = compute_critic_loss_monte_carlo(
                agent=agent,
                batch=batch,
                device=device
            )

            # Backward pass
            optimizer.zero_grad()
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), max_norm=1.0)

            optimizer.step()

            # Update target network (for consistency with online phase)
            agent.update_target_critic(tau=tau)

            # Record loss
            epoch_losses.append(loss.item())
            training_history['batch_losses'].append(loss.item())

            # Update progress bar
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'avg_loss': f"{np.mean(epoch_losses):.4f}"
            })

        # Epoch statistics
        avg_epoch_loss = np.mean(epoch_losses)
        training_history['epoch_losses'].append(avg_epoch_loss)

        print(f"     Epoch {epoch+1} - Avg Loss: {avg_epoch_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}")

        # Save best model
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            print(f"     ✅ New best loss! Saving checkpoint...")

            # Save checkpoint
            checkpoint = {
                'epoch': epoch,
                'critic_state_dict': agent.critic.state_dict(),
                'target_critic_state_dict': agent.target_critic.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'training_history': training_history
            }

            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            torch.save(checkpoint, save_path)

        # Step scheduler
        scheduler.step()

    # Training complete
    print("\n" + "="*80)
    print("CRITIC PRE-TRAINING COMPLETE")
    print("="*80)
    print(f"\nFinal statistics:")
    print(f"  - Best loss: {best_loss:.4f}")
    print(f"  - Final loss: {avg_epoch_loss:.4f}")
    print(f"  - Total batches trained: {len(training_history['batch_losses'])}")
    print(f"\nCheckpoint saved to: {save_path}")

    return agent, training_history


def evaluate_critic(
    agent: ActorCriticAgent,
    buffer: ReplayBuffer,
    num_samples: int = 1000,
    device: str = 'cuda'
) -> Dict:
    """
    Evaluate critic performance on held-out data.

    Args:
        agent: ActorCriticAgent with trained critic
        buffer: ReplayBuffer to sample from
        num_samples: Number of samples to evaluate
        device: Device to use

    Returns:
        Dictionary of evaluation metrics
    """
    agent.critic.eval()

    # Sample batch
    batch = buffer.sample(num_samples)

    # Get predictions
    states = torch.stack([t['state'] for t in batch]).to(device)
    actions = torch.tensor([t['action'] for t in batch], dtype=torch.long).to(device)
    rewards = torch.tensor([t['reward'] for t in batch], dtype=torch.float32).to(device)

    with torch.no_grad():
        # Get Q-values
        q_values = agent.critic(states)
        predicted_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)

        # Calculate metrics
        mse = ((predicted_values - rewards) ** 2).mean().item()
        mae = (predicted_values - rewards).abs().mean().item()

        # Q-value statistics
        q_mean = q_values.mean().item()
        q_std = q_values.std().item()
        q_max = q_values.max().item()
        q_min = q_values.min().item()

    metrics = {
        'mse': mse,
        'mae': mae,
        'q_mean': q_mean,
        'q_std': q_std,
        'q_max': q_max,
        'q_min': q_min,
        'avg_predicted_value': predicted_values.mean().item(),
        'avg_actual_reward': rewards.mean().item()
    }

    agent.critic.train()

    return metrics


if __name__ == '__main__':
    # Pre-train critic
    agent, history = pretrain_critic(
        data_path='data/critic_training_data.pkl',
        save_path='./checkpoints/pretrained_critic.pt',
        num_epochs=5,
        batch_size=256,
        learning_rate=1e-4
    )

    print("\n✅ Critic pre-training complete!")
    print("   Ready for online actor-critic training!")

    # Optional: Evaluate on the training data
    print("\n4. Evaluating critic...")
    with open('data/critic_training_data.pkl', 'rb') as f:
        buffer = pickle.load(f)

    metrics = evaluate_critic(agent, buffer, num_samples=5000)

    print(f"   Evaluation metrics:")
    print(f"   - MSE: {metrics['mse']:.4f}")
    print(f"   - MAE: {metrics['mae']:.4f}")
    print(f"   - Avg Q-value: {metrics['q_mean']:.4f} ± {metrics['q_std']:.4f}")
    print(f"   - Q-value range: [{metrics['q_min']:.4f}, {metrics['q_max']:.4f}]")
    print(f"   - Avg predicted value: {metrics['avg_predicted_value']:.4f}")
    print(f"   - Avg actual reward: {metrics['avg_actual_reward']:.4f}")
