# Training with New Dataset Format

This guide explains how to train models using the new enhanced dataset format generated by `generate_full_dataset.py`.

## Dataset Format

The new dataset is a pickle file containing:
```python
{
    'AAPL': {
        datetime.date(2020, 1, 1): tensor([...]),  # All features for this date
        datetime.date(2020, 1, 2): tensor([...]),
        ...
    },
    'MSFT': { ... },
    ...
}
```

Each daily tensor contains:
- **Base Features**: ~400-500 fundamental + technical features (cross-sectionally normalized)
- **Cross-Sectional Features**: ~50-100 percentile rankings (optional, if enabled)
- **News Embeddings**: 768-dim Nomic AI embeddings

Total: ~1200-1400 features per day

## Quick Start

### 1. Test the Data Loader

```bash
python training/new_data_loader.py your_dataset_complete_dataset.pkl
```

This will:
- Load the dataset
- Build sequence index
- Create train/val split
- Show sample batches

### 2. Start Training

**Simple regression model:**
```bash
python training/train_new_format.py \
    --data your_dataset_complete_dataset.pkl \
    --seq-len 60 \
    --batch-size 32 \
    --hidden-dim 512 \
    --num-layers 6 \
    --epochs 100 \
    --pred-mode regression
```

**Classification model (with bins):**
```bash
python training/train_new_format.py \
    --data your_dataset_complete_dataset.pkl \
    --seq-len 60 \
    --batch-size 32 \
    --hidden-dim 512 \
    --num-layers 6 \
    --epochs 100 \
    --pred-mode classification
```

**With Weights & Biases logging:**
```bash
python training/train_new_format.py \
    --data your_dataset_complete_dataset.pkl \
    --use-wandb \
    --seq-len 60 \
    --epochs 100
```

## Architecture

### Data Loader (`new_data_loader.py`)

**StockSequenceDataset:**
- Loads pickle format
- Creates sequences of length `seq_len` from daily tensors
- Handles train/val split
- Returns:
  - `features`: (batch, seq_len, total_features)
  - `prices`: (batch, num_pred_days) - future price ratios
  - `ticker`: List of ticker strings
  - `date`: List of dates

**StockDataModule:**
- Manages train/val dataloaders
- Provides easy access to feature dimensions
- Handles batching and multiprocessing

### Model (`train_new_format.py`)

**SimpleTransformerPredictor:**
- Input projection: `total_features -> hidden_dim`
- Positional encoding
- Transformer encoder (6 layers default)
- Prediction head (regression or classification)

**Prediction Modes:**
1. **Regression**: Directly predict price ratios (future/current)
2. **Classification**: Predict distribution over 100 bins

## Command Line Arguments

### Data Arguments
```bash
--data PATH              # Path to dataset pickle
--seq-len INT           # Sequence length (default: 60)
--batch-size INT        # Batch size (default: 32)
--num-workers INT       # Data loader workers (default: 4)
```

### Model Arguments
```bash
--hidden-dim INT        # Transformer hidden dim (default: 512)
--num-layers INT        # Number of transformer layers (default: 6)
--num-heads INT         # Attention heads (default: 8)
--dropout FLOAT         # Dropout rate (default: 0.1)
--pred-mode STR         # 'regression' or 'classification'
```

### Training Arguments
```bash
--epochs INT            # Number of epochs (default: 100)
--lr FLOAT             # Learning rate (default: 1e-4)
--optimizer STR        # 'adamw' or 'lion' (default: adamw)
--device STR           # 'cuda' or 'cpu' (default: cuda)
```

### Logging Arguments
```bash
--use-wandb            # Enable Weights & Biases logging
--save-dir PATH        # Checkpoint save directory (default: ./checkpoints)
```

## Model Sizes

### Small Model (Fast training)
```bash
--hidden-dim 256 --num-layers 4 --num-heads 4
# ~10M parameters
```

### Medium Model (Balanced)
```bash
--hidden-dim 512 --num-layers 6 --num-heads 8
# ~40M parameters
```

### Large Model (Best performance)
```bash
--hidden-dim 1024 --num-layers 12 --num-heads 16
# ~200M parameters
```

## Feature Handling

The data loader provides helper methods to split features:

```python
# In your training code
features, prices, tickers, dates = next(iter(train_loader))

# Split into base and news
base_features, news_features = train_dataset.get_feature_splits(features)

# base_features: (batch, seq_len, ~600)
# news_features: (batch, seq_len, 768)
```

This is useful if you want to:
- Process base features and news separately
- Apply different architectures to different feature types
- Experiment with feature importance

## What Changed From Old Format

### Old Format
```python
# Required two separate files
bulk_prices = {company: {date: tensor([price_data])}}
summaries = {company: tensor([summary])}

# Sequences built manually
# Features not normalized consistently
```

### New Format
```python
# Single file with everything
data = {ticker: {date: tensor([all_features])}}

# Features are:
# - Cross-sectionally normalized (fundamentals)
# - Temporally normalized (technical indicators)
# - Include news embeddings
# - Include cross-sectional rankings
```

### Migration Notes

If you have old training code:

1. **Data Loading**: Replace `GenerateDataDict` with `StockSequenceDataset`
2. **Input Dimension**: Update model to accept ~1200+ features instead of ~100
3. **No Separate Summary**: Everything is in one tensor now
4. **Better Normalization**: Features already normalized properly

## Advanced Usage

### Custom Model

Create your own model using the data loader:

```python
from training.new_data_loader import StockDataModule

# Load data
dm = StockDataModule('your_dataset.pkl')

# Get feature dimensions
input_dim = dm.total_features  # e.g., 1268
base_dim = dm.base_features     # e.g., 500
news_dim = dm.news_dim          # 768

# Create custom model
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Use input_dim for first layer
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 512),
            # ... your architecture
        )

    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        # ... your forward pass
        return predictions
```

### Different Prediction Horizons

Modify `pred_days` in the data module:

```python
dm = StockDataModule(
    dataset_path='data.pkl',
    pred_days=[1, 3, 7, 14, 30]  # Predict 1d, 3d, 7d, 14d, 30d ahead
)
```

### Feature Selection

If you want to exclude certain features:

```python
class FeatureSubsetDataset(StockSequenceDataset):
    def __getitem__(self, idx):
        features, prices, ticker, date = super().__getitem__(idx)

        # Only use base features (no news)
        features = features[:, :self.base_features]

        # Or only use news
        # features = features[:, self.base_features:]

        return features, prices, ticker, date
```

## Troubleshooting

### Out of Memory

**GPU OOM during training:**
```bash
# Reduce batch size
--batch-size 16

# Or smaller model
--hidden-dim 256 --num-layers 4
```

**RAM OOM during data loading:**
```bash
# Reduce workers
--num-workers 2

# Or process subset of data first
```

### NaN/Inf in Features

The data loader automatically handles NaN/Inf via `set_nan_inf()`, which replaces them with 0.

If you still see issues:
1. Check your dataset was generated with proper normalization
2. Verify cross-sectional normalization was enabled
3. Ensure news embeddings are present

### Slow Data Loading

```bash
# Increase workers (if you have RAM)
--num-workers 8

# Or reduce sequence length
--seq-len 30
```

## Next Steps

1. **Experiment with architectures**: Try different model sizes
2. **Tune hyperparameters**: Learning rate, dropout, etc.
3. **Try different prediction modes**: Regression vs classification
4. **Feature analysis**: See which features matter most
5. **Ensemble models**: Train multiple models and ensemble predictions

## Support

For issues or questions:
1. Check the data loader test: `python training/new_data_loader.py your_data.pkl`
2. Verify dataset format: Should be `{ticker: {date: tensor}}`
3. Check feature dimensions match what the model expects
