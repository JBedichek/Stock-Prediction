#!/usr/bin/env python3
"""
Enhanced Automated Daily Stock Prediction Pipeline

Features:
1. Automatically detects missing date range (from latest in dataset to today)
2. Fetches data for ALL missing dates
3. Applies exact same normalization as training (cross-sectional for fundamentals, temporal for prices)
4. Appends all missing data in one batch
5. Runs inference on updated dataset

Usage:
    python auto_daily_predictions_enhanced.py --top-k 10
"""

import torch
import argparse
import h5py
import sys
import os
import numpy as np
import pandas as pd
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm

# Add parent directory to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.utils import pic_load, save_pickle
from data_scraping.cross_sectional_normalizer import CrossSectionalNormalizer
from inference.predict_current_day import CurrentDayPredictor, print_stock_recommendations, save_recommendations


class EnhancedDataUpdater:
    """Handles multi-date updates with proper normalization matching training."""

    def __init__(self, dataset_path: str, prices_path: Optional[str] = None):
        """
        Args:
            dataset_path: Path to main HDF5 dataset
            prices_path: Path to prices HDF5 (optional)
        """
        self.dataset_path = dataset_path
        self.prices_path = prices_path
        self.dataset_exists = Path(dataset_path).exists()
        self.normalizer = CrossSectionalNormalizer()

    def get_latest_date(self) -> Optional[date]:
        """Get the latest date in the dataset with REAL data (not forward-filled)."""
        if not self.dataset_exists:
            return None

        with h5py.File(self.dataset_path, 'r') as h5f:
            # Check multiple tickers to find the true latest date
            sample_tickers = list(h5f.keys())[:10]  # Check first 10 tickers
            forward_filled_count = 0

            for ticker in sample_tickers:
                dates_bytes = h5f[ticker]['dates'][:]
                dates = [d.decode('utf-8') for d in dates_bytes]
                features = h5f[ticker]['features'][:]

                if len(dates) < 2:
                    continue

                # Check if last date is forward-filled
                last_features = features[-1, :]
                second_last_features = features[-2, :]

                if np.allclose(last_features, second_last_features, rtol=1e-5):
                    forward_filled_count += 1

            # Get dates from first ticker
            first_ticker = sample_tickers[0]
            dates_bytes = h5f[first_ticker]['dates'][:]
            all_dates = sorted([d.decode('utf-8') for d in dates_bytes])

            # If most tickers have forward-filled data, use second-to-last date
            if forward_filled_count >= len(sample_tickers) * 0.5:
                latest_str = all_dates[-2]  # Use second-to-last
                print(f"  ‚ö†Ô∏è  Latest date ({all_dates[-1]}) is forward-filled")
                print(f"  ‚úÖ Using last real data date: {latest_str}")
            else:
                latest_str = all_dates[-1]

            latest = datetime.strptime(latest_str, '%Y-%m-%d').date()
            return latest

    def get_missing_date_range(self) -> Tuple[List[date], str]:
        """
        Detect missing dates between latest in dataset and today.

        Returns:
            Tuple of (list_of_missing_dates, status_message)
        """
        if not self.dataset_exists:
            return [], "Dataset doesn't exist - need initial creation"

        latest_date = self.get_latest_date()
        today = date.today()

        # Generate all trading days between latest and today
        missing_dates = []
        current = latest_date + timedelta(days=1)

        while current <= today:
            # Skip weekends (Saturday=5, Sunday=6)
            if current.weekday() < 5:
                missing_dates.append(current)
            current += timedelta(days=1)

        if len(missing_dates) == 0:
            return [], f"Dataset is up-to-date (latest: {latest_date})"

        return missing_dates, f"Found {len(missing_dates)} missing dates ({missing_dates[0]} to {missing_dates[-1]})"

    def fetch_multi_date_data(self, date_range: List[date]) -> Dict[date, Dict[str, pd.DataFrame]]:
        """
        Fetch raw data for multiple dates.

        Args:
            date_range: List of dates to fetch

        Returns:
            Dict of {date: {ticker: DataFrame}} with raw features (not normalized yet)
        """
        print(f"\n{'='*80}")
        print(f"FETCHING DATA FOR {len(date_range)} DATES")
        print(f"{'='*80}")
        print(f"  Date range: {date_range[0]} to {date_range[-1]}")

        # Import data fetching utilities
        try:
            from data_scraping.yfinance_price_scraper import fetch_date_range_prices
            from data_scraping.fmp_enhanced_processor import EnhancedFMPDataProcessor
            from data_scraping.news_embedder import fetch_date_range_news
        except ImportError:
            print("‚ö†Ô∏è  Data fetching utilities not found")
            print("   Using mock data for testing...")
            return self._fetch_mock_multi_date_data(date_range)

        # Fetch data for entire date range
        all_data = {}

        # 1. Fetch prices for all dates
        print(f"\nüìà Fetching prices for {len(date_range)} dates...")
        try:
            prices_df = fetch_date_range_prices(
                start_date=date_range[0],
                end_date=date_range[-1]
            )
            print(f"  ‚úÖ Got prices for {len(prices_df.columns)} tickers")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Failed to fetch prices: {e}")
            prices_df = None

        # 2. Fetch fundamentals (usually quarterly, so we'll forward-fill)
        print(f"\nüìä Fetching fundamentals...")
        try:
            fundamentals = self._fetch_fundamentals_batch(date_range)
            print(f"  ‚úÖ Got fundamentals for {len(fundamentals)} tickers")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Failed to fetch fundamentals: {e}")
            fundamentals = None

        # 3. Fetch news embeddings for each date
        print(f"\nüì∞ Fetching news embeddings...")
        try:
            news_by_date = fetch_date_range_news(date_range)
            print(f"  ‚úÖ Got news for {len(news_by_date)} dates")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Failed to fetch news: {e}")
            news_by_date = {}

        # 4. Combine into per-date DataFrames
        print(f"\nüî® Processing data for each date...")
        for target_date in tqdm(date_range, desc="  Processing"):
            date_str = target_date.strftime('%Y-%m-%d')

            # Get prices for this date
            if prices_df is not None and date_str in prices_df.index:
                date_prices = prices_df.loc[date_str]
            else:
                continue

            # Build DataFrame for each ticker on this date
            ticker_dfs = {}
            for ticker in date_prices.index:
                try:
                    df = self._build_ticker_features(
                        ticker=ticker,
                        target_date=target_date,
                        prices_df=prices_df,
                        fundamentals=fundamentals.get(ticker) if fundamentals else None,
                        news=news_by_date.get(target_date)
                    )
                    if df is not None and not df.empty:
                        ticker_dfs[ticker] = df
                except Exception as e:
                    continue

            all_data[target_date] = ticker_dfs

        print(f"  ‚úÖ Processed {len(all_data)} dates with an average of {np.mean([len(v) for v in all_data.values()]):.0f} tickers per date")

        return all_data

    def _build_ticker_features(self,
                               ticker: str,
                               target_date: date,
                               prices_df: pd.DataFrame,
                               fundamentals: Optional[Dict],
                               news: Optional[torch.Tensor]) -> Optional[pd.DataFrame]:
        """
        Build feature DataFrame for a single ticker on a single date.

        Args:
            ticker: Stock ticker
            target_date: Date to build features for
            prices_df: DataFrame with all prices (multi-date)
            fundamentals: Dict with fundamental data
            news: News embedding tensor

        Returns:
            DataFrame with features (one row = one date)
        """
        date_str = target_date.strftime('%Y-%m-%d')

        # Get price data for this ticker
        if date_str not in prices_df.index:
            return None

        price_data = prices_df.loc[date_str, ticker] if ticker in prices_df.columns else None
        if price_data is None or pd.isna(price_data):
            return None

        # Build feature dict
        features = {}

        # Add price features (will be temporally normalized later)
        features['close'] = price_data.get('Close', price_data) if isinstance(price_data, dict) else price_data
        features['open'] = price_data.get('Open', features['close']) if isinstance(price_data, dict) else features['close']
        features['high'] = price_data.get('High', features['close']) if isinstance(price_data, dict) else features['close']
        features['low'] = price_data.get('Low', features['close']) if isinstance(price_data, dict) else features['close']
        features['volume'] = price_data.get('Volume', 0) if isinstance(price_data, dict) else 0

        # Add fundamental features (will be cross-sectionally normalized later)
        if fundamentals:
            for key, val in fundamentals.items():
                if isinstance(val, (int, float)) and not pd.isna(val):
                    features[f'fundamental_{key}'] = val

        # Create DataFrame with this single date
        df = pd.DataFrame([features], index=[target_date])

        return df

    def _fetch_fundamentals_batch(self, date_range: List[date]) -> Dict[str, Dict]:
        """Fetch fundamentals for all tickers (forward-filled for daily data)."""
        # In practice, fundamentals are quarterly, so we fetch latest and forward-fill
        # This is a placeholder - implement based on your data source
        return {}

    def _fetch_mock_multi_date_data(self, date_range: List[date]) -> Dict[date, Dict[str, pd.DataFrame]]:
        """Generate mock data for testing."""
        print(f"‚ö†Ô∏è  Using mock data (copying from existing dataset)")

        if not self.dataset_exists:
            return {}

        all_data = {}

        with h5py.File(self.dataset_path, 'r') as h5f:
            tickers = list(h5f.keys())[:100]  # Sample 100 tickers

            for target_date in date_range:
                ticker_dfs = {}

                for ticker in tickers:
                    # Create mock DataFrame with one row
                    features = h5f[ticker]['features'][-1, :]
                    df = pd.DataFrame([features], index=[target_date])
                    ticker_dfs[ticker] = df

                all_data[target_date] = ticker_dfs

        return all_data

    def normalize_multi_date_data(self,
                                  new_data: Dict[date, Dict[str, pd.DataFrame]],
                                  use_existing_stats: bool = True) -> Dict[date, Dict[str, torch.Tensor]]:
        """
        Normalize multi-date data using the SAME strategy as training.

        This is CRITICAL: We must match training normalization exactly!

        Args:
            new_data: Dict of {date: {ticker: DataFrame}}
            use_existing_stats: Use stats from existing dataset for temporal normalization

        Returns:
            Dict of {date: {ticker: features_tensor}}
        """
        print(f"\n{'='*80}")
        print("NORMALIZING DATA (MATCHING TRAINING)")
        print(f"{'='*80}")

        # Step 1: Reshape data for cross-sectional normalization
        # We need: {ticker: DataFrame_with_all_dates}
        print(f"\nüìä Step 1: Preparing for cross-sectional normalization...")

        all_stocks_dfs = {}
        for target_date, ticker_dfs in new_data.items():
            for ticker, df in ticker_dfs.items():
                if ticker not in all_stocks_dfs:
                    all_stocks_dfs[ticker] = df
                else:
                    all_stocks_dfs[ticker] = pd.concat([all_stocks_dfs[ticker], df])

        print(f"  ‚úÖ Organized {len(all_stocks_dfs)} tickers")

        # Step 2: Get feature columns and categorize them
        all_columns = set()
        for df in all_stocks_dfs.values():
            all_columns.update(df.columns)
        all_columns = sorted(list(all_columns))

        cross_sectional_cols, temporal_cols, no_norm_cols = self.normalizer.categorize_columns(all_columns)

        print(f"\nüìã Feature categorization:")
        print(f"  Cross-sectional (fundamentals): {len(cross_sectional_cols)}")
        print(f"  Temporal (prices/technicals): {len(temporal_cols)}")
        print(f"  No normalization: {len(no_norm_cols)}")

        # Step 3: Apply cross-sectional normalization
        # For each cross-sectional feature, normalize across stocks for each date
        print(f"\nüîÑ Step 2: Cross-sectional normalization...")
        for col in tqdm(cross_sectional_cols, desc="  Normalizing"):
            normalized_series = self.normalizer.normalize_cross_sectional(all_stocks_dfs, col)
            for ticker in all_stocks_dfs.keys():
                if ticker in normalized_series:
                    all_stocks_dfs[ticker][col] = normalized_series[ticker]

        # Step 4: Apply temporal normalization
        # For each stock, normalize temporal features using its own historical stats
        print(f"\nüîÑ Step 3: Temporal normalization...")

        if use_existing_stats:
            # Load stats from existing dataset
            temporal_stats = self._load_temporal_stats_from_dataset()
        else:
            temporal_stats = {}

        for ticker in tqdm(all_stocks_dfs.keys(), desc="  Normalizing"):
            for col in temporal_cols:
                if col in all_stocks_dfs[ticker].columns:
                    if ticker in temporal_stats and col in temporal_stats[ticker]:
                        # Use existing stats (from full history)
                        mean = temporal_stats[ticker][col]['mean']
                        std = temporal_stats[ticker][col]['std']
                        series = all_stocks_dfs[ticker][col]
                        if std > 1e-8:
                            all_stocks_dfs[ticker][col] = (series - mean) / std
                        else:
                            all_stocks_dfs[ticker][col] = 0
                    else:
                        # Fallback: normalize using just these dates
                        all_stocks_dfs[ticker][col] = self.normalizer.normalize_temporal(
                            all_stocks_dfs[ticker][col]
                        )

        # Step 5: Convert to tensors
        print(f"\nüîß Step 4: Converting to tensors...")
        normalized_data = {}

        for target_date in new_data.keys():
            normalized_data[target_date] = {}

            for ticker in new_data[target_date].keys():
                if ticker in all_stocks_dfs:
                    # Get this date's row
                    if target_date in all_stocks_dfs[ticker].index:
                        row = all_stocks_dfs[ticker].loc[target_date]
                        tensor = torch.tensor(row.values, dtype=torch.float32)

                        # Replace NaN/Inf with 0
                        tensor = torch.nan_to_num(tensor, nan=0.0, posinf=0.0, neginf=0.0)

                        normalized_data[target_date][ticker] = tensor

        print(f"  ‚úÖ Created normalized tensors for {len(normalized_data)} dates")

        return normalized_data

    def _load_temporal_stats_from_dataset(self) -> Dict[str, Dict[str, Dict[str, float]]]:
        """
        Load temporal normalization statistics from existing dataset.

        Returns:
            Dict of {ticker: {feature: {'mean': X, 'std': Y}}}
        """
        print(f"  üìä Loading temporal stats from existing dataset...")

        if not self.dataset_exists:
            return {}

        stats = {}

        with h5py.File(self.dataset_path, 'r') as h5f:
            # We need to infer feature names - they're not stored
            # For simplicity, compute stats from existing data
            sample_ticker = list(h5f.keys())[0]
            num_features = h5f[sample_ticker]['features'].shape[1]

            # Load a sample to understand feature layout
            # In practice, you'd need feature names mapping
            # For now, we'll just compute stats per feature index

            for ticker in tqdm(list(h5f.keys()), desc="    Loading stats", leave=False):
                try:
                    features_array = h5f[ticker]['features'][:]  # (num_dates, num_features)

                    stats[ticker] = {}
                    for feat_idx in range(num_features):
                        feat_data = features_array[:, feat_idx]

                        # Compute mean and std
                        valid_data = feat_data[~np.isnan(feat_data) & ~np.isinf(feat_data)]
                        if len(valid_data) > 0:
                            stats[ticker][f'feature_{feat_idx}'] = {
                                'mean': float(np.mean(valid_data)),
                                'std': float(np.std(valid_data))
                            }
                except Exception:
                    continue

        print(f"    ‚úÖ Loaded stats for {len(stats)} tickers")

        return stats

    def append_multi_date_data(self, normalized_data: Dict[date, Dict[str, torch.Tensor]]):
        """
        Append multiple dates of normalized data to HDF5 dataset.

        Args:
            normalized_data: Dict of {date: {ticker: features_tensor}}
        """
        print(f"\n{'='*80}")
        print(f"UPDATING DATASET WITH {len(normalized_data)} DATES")
        print(f"{'='*80}")

        # Sort dates
        sorted_dates = sorted(normalized_data.keys())

        with h5py.File(self.dataset_path, 'a') as h5f:
            # Get all unique tickers
            all_tickers = set()
            for date_data in normalized_data.values():
                all_tickers.update(date_data.keys())

            for ticker in tqdm(all_tickers, desc="  Updating tickers"):
                if ticker not in h5f:
                    # Create new ticker group
                    grp = h5f.create_group(ticker)

                    # Collect features for all dates for this ticker
                    ticker_features = []
                    ticker_dates = []

                    for target_date in sorted_dates:
                        if ticker in normalized_data[target_date]:
                            ticker_features.append(normalized_data[target_date][ticker].numpy())
                            ticker_dates.append(target_date.strftime('%Y-%m-%d').encode('utf-8'))

                    if len(ticker_features) > 0:
                        features_array = np.stack(ticker_features, axis=0)
                        grp.create_dataset('features', data=features_array,
                                         maxshape=(None, features_array.shape[1]), dtype='float32')
                        grp.create_dataset('dates', data=np.array(ticker_dates),
                                         maxshape=(None,), dtype='S10')
                else:
                    # Append to existing ticker
                    grp = h5f[ticker]

                    # Collect new features
                    new_features = []
                    new_dates = []

                    existing_dates = [d.decode('utf-8') for d in grp['dates'][:]]

                    for target_date in sorted_dates:
                        date_str = target_date.strftime('%Y-%m-%d')

                        if ticker in normalized_data[target_date]:
                            if date_str not in existing_dates:
                                new_features.append(normalized_data[target_date][ticker].numpy())
                                new_dates.append(date_str.encode('utf-8'))

                    if len(new_features) > 0:
                        # Resize and append
                        old_size = grp['features'].shape[0]
                        new_size = old_size + len(new_features)

                        grp['features'].resize((new_size, grp['features'].shape[1]))
                        grp['dates'].resize((new_size,))

                        # Add new data
                        new_features_array = np.stack(new_features, axis=0)
                        grp['features'][old_size:new_size, :] = new_features_array
                        grp['dates'][old_size:new_size] = np.array(new_dates)

        print(f"  ‚úÖ Dataset updated with {len(sorted_dates)} new dates")

        # Update prices file
        if self.prices_path and Path(self.prices_path).exists():
            self._append_prices(normalized_data, sorted_dates)

    def _append_prices(self, normalized_data: Dict[date, Dict[str, torch.Tensor]], sorted_dates: List[date]):
        """Append to prices HDF5."""
        print(f"\nüìä Updating prices file...")

        with h5py.File(self.prices_path, 'a') as h5f:
            for ticker in tqdm(set().union(*[set(d.keys()) for d in normalized_data.values()]), desc="  Updating"):
                # Extract price (first element of features tensor)
                ticker_prices = []
                ticker_dates = []

                for target_date in sorted_dates:
                    if ticker in normalized_data[target_date]:
                        price = normalized_data[target_date][ticker][0].item()
                        ticker_prices.append(price)
                        ticker_dates.append(target_date.strftime('%Y-%m-%d').encode('utf-8'))

                if len(ticker_prices) == 0:
                    continue

                if ticker not in h5f:
                    grp = h5f.create_group(ticker)
                    grp.create_dataset('prices', data=np.array(ticker_prices),
                                     maxshape=(None,), dtype='float32')
                    grp.create_dataset('dates', data=np.array(ticker_dates),
                                     maxshape=(None,), dtype='S10')
                else:
                    grp = h5f[ticker]
                    old_size = grp['prices'].shape[0]
                    new_size = old_size + len(ticker_prices)

                    grp['prices'].resize((new_size,))
                    grp['dates'].resize((new_size,))

                    grp['prices'][old_size:new_size] = np.array(ticker_prices)
                    grp['dates'][old_size:new_size] = np.array(ticker_dates)

        print(f"  ‚úÖ Prices updated")


def run_enhanced_pipeline(args):
    """Run the enhanced automated pipeline with multi-date updates."""

    print(f"\n{'='*80}")
    print(f"ENHANCED AUTOMATED STOCK PREDICTION PIPELINE")
    print(f"{'='*80}")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # Step 1: Detect missing dates
    updater = EnhancedDataUpdater(
        dataset_path=args.dataset,
        prices_path=args.prices
    )

    missing_dates, status = updater.get_missing_date_range()
    print(f"üìã Dataset status: {status}")

    if len(missing_dates) > 0 and not args.skip_update:
        print(f"\n‚ö†Ô∏è  Dataset is missing {len(missing_dates)} dates")
        print(f"   Will fetch and append: {missing_dates[0]} to {missing_dates[-1]}")

        try:
            # Step 2: Fetch data for all missing dates
            raw_data = updater.fetch_multi_date_data(missing_dates)

            if len(raw_data) == 0:
                print(f"\n‚ö†Ô∏è  No data fetched - using existing dataset")
            else:
                # Step 3: Normalize data (matching training!)
                normalized_data = updater.normalize_multi_date_data(raw_data)

                # Step 4: Append to dataset
                updater.append_multi_date_data(normalized_data)

                print(f"\n‚úÖ Successfully updated dataset with {len(missing_dates)} dates!")

        except Exception as e:
            print(f"\n‚ö†Ô∏è  Data update failed: {e}")
            print(f"   Continuing with existing dataset...")
            import traceback
            traceback.print_exc()

    # Step 5: Run predictions
    print(f"\n{'='*80}")
    print("RUNNING PREDICTIONS")
    print(f"{'='*80}")

    predictor = CurrentDayPredictor(
        dataset_path=args.dataset,
        prices_path=args.prices,
        model_path=args.model,
        bin_edges_path=args.bin_edges,
        device=args.device,
        batch_size=args.batch_size,
        compile_model=args.compile
    )

    # Get latest date
    prediction_date = predictor.get_latest_date()
    print(f"üìÖ Making predictions for: {prediction_date}")

    # Run predictions
    horizon_map = {0: 1, 1: 5, 2: 10, 3: 20}
    horizon_days = horizon_map.get(args.horizon_idx, 5)

    stocks = predictor.predict_all_stocks(
        date=prediction_date,
        horizon_idx=args.horizon_idx,
        confidence_percentile=args.confidence_percentile
    )

    # Print recommendations
    print_stock_recommendations(
        stocks=stocks,
        top_k=args.top_k,
        horizon_days=horizon_days,
        date=prediction_date
    )

    # Save results
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"daily_predictions_{timestamp}.pt"

    save_recommendations(
        stocks=stocks,
        output_path=output_file,
        date=prediction_date,
        horizon_days=horizon_days,
        top_k=args.top_k
    )

    # Save text summary
    text_output = f"daily_predictions_{timestamp}.txt"
    with open(text_output, 'w') as f:
        f.write(f"Daily Stock Predictions - {prediction_date}\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"{'='*80}\n\n")

        f.write(f"Top {args.top_k} Stocks to Buy:\n\n")
        for rank, (ticker, expected_return, confidence, price) in enumerate(stocks[:args.top_k], 1):
            expected_pct = (expected_return - 1.0) * 100
            f.write(f"{rank}. {ticker}: +{expected_pct:.2f}% ")
            f.write(f"(confidence: {confidence:.4f}, price: ${price:.2f})\n")

        f.write(f"\nHolding period: {horizon_days} trading days\n")

    print(f"\nüíæ Saved text summary to: {text_output}")

    print(f"\n{'='*80}")
    print("‚úÖ PIPELINE COMPLETE")
    print(f"{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Enhanced automated pipeline with multi-date updates and proper normalization',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Dataset paths
    parser.add_argument('--dataset', type=str, default='data/all_complete_dataset.h5',
                       help='Path to HDF5 dataset')
    parser.add_argument('--prices', type=str, default='data/actual_prices.h5',
                       help='Path to prices HDF5')

    # Model paths
    parser.add_argument('--model', type=str, default='checkpoints/best_model.pt',
                       help='Path to model checkpoint')
    parser.add_argument('--bin-edges', type=str, default='data/adaptive_bin_edges.pt',
                       help='Path to bin edges')

    # Prediction parameters
    parser.add_argument('--top-k', type=int, default=10,
                       help='Number of stocks to recommend')
    parser.add_argument('--horizon-idx', type=int, default=1,
                       help='Prediction horizon (0=1d, 1=5d, 2=10d, 3=20d)')
    parser.add_argument('--confidence-percentile', type=float, default=0.6,
                       help='Confidence filter threshold')

    # Update options
    parser.add_argument('--skip-update', action='store_true',
                       help='Skip dataset update, just run predictions')

    # Inference options
    parser.add_argument('--batch-size', type=int, default=256,
                       help='Inference batch size')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device (cuda or cpu)')
    parser.add_argument('--compile', action='store_true',
                       help='Use torch.compile for faster inference')

    args = parser.parse_args()

    run_enhanced_pipeline(args)


if __name__ == '__main__':
    main()
